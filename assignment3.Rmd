---
title: |
  <center> University of Edinburgh, School of Mathematics </center>
  <center> Statistical Research Skills </center>
  <center> Assignment 3 - Simulation Report</center>
author: "Johnny Lee"
date: '26th Apr 2022 '
output:
  pdf_document : 
    fig_caption: true
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
Sys.setlocale("LC_ALL", "English")
library(tidyverse)
library(gss)
```

# 1. Introduction

This report consists of two main parts. In the first part, we aim to compute one-shot experiment on density function, `density()` also known as the kernel density estimator. In addition to that, we will compare with its other competitors such as orthogonal series estimator and penalised kernel density estimator. In the second part, we will conduct a Monte Carlo simulation study for different sample sizes on the previously suggested methods. Thus, we will evaluate the integrated squared error (ISE) for different cases of sample sizes.

# 2. Preliminary Experiment
This section consist of the first part of the report where we conduct the preliminary experiments with 3 different density estimators. We will introduce the detailed methodology with its mathematical equation. With random data generation from normal and beta distribution, we will conduct one-shot experiment on these estimators.

## 2.1 Methodology

### 2.1.1 Kernel Denisty Estimator
Let $X_1,\dots,X_n\overset{\text{iid}}{\sim}f$. The kernel density estimator of $f$ is defined as

$$\hat{f}(x)=\frac{1}{n}\sum^{n}_{i=1}K_h(x-X_i)$$
where $K_h(x)=\frac{K(x)}{h}$, $K$ is a kernel and $h>0$ is a parameter controlling smoothness of the estimate. `density()` function performs univariate density estimate with various kernels but we will choose the default kernel and bandwidth in this experiment.

### 2.1.2 Orthogonal Series Estimator 
With reference to (Kreyszig, 1991)[5], Kreyszig introduced the orthogonal series estimator using normalised Hermite polynomials. These polynomials form a orthonormal sequence for the univariate case as below,
$$\hat{f}(x)=\frac{1}{(2^ii!\sqrt{x})^{\frac{1}{2}}}\exp(-x^2/2)H_i(x)$$
and 
$$
H_0(x)=1, \quad H_i(x)=(-1)^i\exp(x^2)\frac{d^i}{dx^i}\exp(-x^2)
$$
where $H_i(x)$ is called the \textit{Hermite polynomial of order $i$}. Furthermore, we can further simplify the process as Kreyszig showed its properties as below.
$$
H_{i+1}(x)=2xH_i(x)-H'_i(x), \quad H'_{i}(x)=2iH_{i-1}(x)
$$
Based on the properties above, we define a new function known as `OS_Hermite()` to iterate the process to obtain the series of estimation.

```{r}
# The implementation of the orthogonal series estimator, using Hermite series
OS_Hermite <- function(x, data){
  # The Hermite polynomials
  # Utilize the recurrence formula "H_n(x) = 2x*H_(n-1)(x) - 2(n-1)*H_(n-2)(x) (n>=2)"
  # to calculate the Hermite polynomials
  hermite <- function(x, m){
    if(m == 0){
      1
    }
    else if(m == 1){2*x}
    else{
      a <- 1
      b <- 2*x
      for(i in 2:m){
        c <- 2*x*b - 2*(i - 1)*a
        a <- b
        b <- c
      }
      c
    }
  }
  # The normalized Hermite functions
  phi <- function(x, m){
    (hermite(x, m)/exp((x^2)/2))/sqrt((2^m)*factorial(m)*sqrt(pi))
  }
  n <- length(data)
  value = 0
  for(m in 0:30){
    c <- sum(sapply(data, phi, m = m))/n
    value <- value + c*phi(x, m)
  }
  value
}
```

### 2.1.3 Penalised Kernel Density Estimator
(Kauermann et al, 2009)[4] introduces the penalised likelihood 

$$
\hat{f}(x)=\sum^{m}_{n=-m}c_n\phi_n(x),
$$
where $\phi_n(x)$ is the basis densities. Then the weight $c_n$ is parameterised as follow
$$
c_n(y)=\frac{\exp(\beta_n)}{\sum^{m}_{n=-m}\exp(\beta_n)}
$$
with $\beta_0=0$ and $\pmb{\beta} = \beta_{-m}, \dots, \beta_{-1}, \beta_1, \dots\beta_m$ so that $\int f(x)dx=1$. 

(Deng et al, 2011)[1] then further suggested the simplified kernel approach 
$$
\hat{f}(x)=\frac{1}{m}\sum^{m}_{n=1}K\bigg(\frac{x-\mu_n}{h}\bigg)
$$
and introduced a package `gss`, (Gu, 2011)[3] which uses a penalised likelihood approach for nonparametric density estimation. With the functions `ssden()` and `dssden()`, we can estimate the kernel density.

## 2.2. Data Generating Process

```{r}
# Generate data for the experiment
n <- 100
set.seed(1)
# The sample one
sample1 <- rnorm(n, mean = 0, sd = 1)
#The sample two
sample2 <- rbeta(n, shape1 = 2, shape2 = 4)
```

We generated two random distributions, normal distribution and beta distribution. For the first distribution, we used`rnorm()` and setting $\mu$ and $\sigma$ as $0$ and $1$ respectively. Then the second distribution takes $\alpha$ and $\beta$ as $2$ and $4$. These values are chosen randomly and no specific reason and we are generating the data in a random process

## 2.3. One-shot Experiments 
```{r fig.height=7.5, fig.width=12, fig.cap="One-shot Experiment on Normal distribution"}
set.seed(1)
# The density functions for the distributions of the simulated data
# The density function for the distribution of sample one
density1 <- function(x){
  mu <- 0; sigma <- 1
  exp(-(x - mu)^2/(2*sigma^2))/(sqrt(2*pi)*sigma)
}
# Create the visualizations to illustrate the performances of different density estimators
sample1.fit <- ssden( ~ sample1, domain = data.frame(sample1))
# The visualization based on sample one
xx <- seq(min(sample1), max(sample1), len = n)
# pdf("Part1_fig1.pdf")
hist(sample1, freq = FALSE, main = "",
     xlab = "X", ylab = "Density", xlim = c(-3, 3), ylim = c(0, 0.6))
lines(density(sample1, from = min(sample1), to = max(sample1)), 
      col = "green", lwd = 1.6)
lines(xx, density1(xx), type = "l", col = "black", lty = 2, lwd = 1.6)
# axis(side = 1, at = seq(min(sample1), max(sample1), n))
lines(xx, sapply(xx, OS_Hermite, data = sample1), col = "red", lwd = 1.6)
lines(xx, dssden(sample1.fit, xx), type = "l", col = "orange", lwd = 1.6)
legend(x = "topright", 
       legend = c("True Density", 
                  "Kernel Density Estimator",
                  "Orthogonal Series Estimator",
                  "Penalised Kernal Density Estimator"),
       lty = c(2, 1, 1, 1),
       col = c("black", "green", "red", "orange"),
       lwd = 1.8, bty = "n")
# dev.off()
```

```{r fig.height=7.5, fig.width=12, fig.cap="One-shot Experiment on Beta distribution"}
# The visualization based on sample two
# The density function for the distribution of sample two
density2 <- function(x){
  alpha <- 2; beta <- 4
  x^(alpha - 1)*(1 - x)^(beta - 1)/beta(alpha, beta)
}
sample2.fit <- ssden( ~ sample2, domain = data.frame(sample2))
xx <- seq(min(sample2), max(sample2), len = n)
hist(sample2, freq = FALSE, main="",
     xlab = "X", ylab = "Density", xlim = c(0, 0.8), ylim = c(0, 3.8))
lines(density(sample2, from = min(sample2), to = max(sample2)), 
      col = "green", lwd = 1.6)
lines(xx, density2(xx), type = "l", col = "black", lty = 2, lwd = 1.6)
# axis(side = 1, at = seq(min(sample2), max(sample2), n))
lines(xx, sapply(xx, OS_Hermite, data = sample2), col = "red", lwd = 1.6)
lines(xx, dssden(sample2.fit, xx), type = "l", col = "orange", lwd = 1.6)
legend(x = "topright", 
       legend = c("True Density", 
                  "Kernel Density Estimator",
                  "Orthogonal Series Estimator",
                  "Penalised Kernal Density Estimator"),
       lty = c(2, 1, 1, 1),
       col = c("black", "green", "red", "orange"), 
       lwd = 1.8, bty = "n")
```

# 4. Monte Carlo Simulation Study


```{r}
set.seed(1)
# Initialize some variables
mu <- 0; sigma <- 1
repeat.no <- 10
n <- c(250, 500, 1000)
df <- 5
ise <- list()
ise$second <- ise$first <- data.frame("kernel" = rep(0, repeat.no), 
                                      "OS" = rep(0, repeat.no),
                                      "pkd" = rep(0, repeat.no))
ise$third <- ise$first

# The density function of the distribution of the sample
density_sam1 = function(x){
  exp(-(x - mu)^2/(2*sigma^2))/(sqrt(2*pi)*sigma)
}
# The Monte Carlo Simulation study for sample size n=250
for(i in 1:repeat.no){
  sample1 <- rnorm(n[1], mean = mu, sd = sigma)
  sample.fit <- ssden( ~ sample1, domain = data.frame(sample1))
  # The calculation of the ISE for the kernel density estimator
  ker_den <- density(sample1)
  interval <- ker_den$x[-1] - ker_den$x[-length(ker_den$x)]
  xx <- (ker_den$x[-1] + ker_den$x[-length(ker_den$x)])/2
  yy <- (ker_den$y[-1] + ker_den$y[-length(ker_den$y)])/2
  ise$first$kernel[i] <- sum((yy - density_sam1(xx))^2*interval)
  
  # The calculation of the ISE for the penalised kernal density estimator
  ise$first$OS[i] <- integrate(function(x){
    (OS_Hermite(x, data = sample1) - density_sam1(x))^2},
    lower = min(sample1), upper = max(sample1))$value 
  
  ise$first$pkd[i] <- integrate(function(sample1){
    (dssden(sample.fit, sample1) - dnorm(sample1, mean = 0, sd = 1))^2}, 
    lower = min(sample1), upper = max(sample1))$value
}

# The Monte Carlo Simulation study for sample size n=500
for(i in 1:repeat.no){
  sample1 <- rnorm(n[2], mean = mu, sd = sigma)
  sample.fit <- ssden( ~ sample1, domain = data.frame(sample1))
  # The calculation of the ISE for the kernel density estimator
  ker_den <- density(sample1)
  interval <- ker_den$x[-1] - ker_den$x[-length(ker_den$x)]
  xx <- (ker_den$x[-1] + ker_den$x[-length(ker_den$x)])/2
  yy <- (ker_den$y[-1] + ker_den$y[-length(ker_den$y)])/2
  ise$second$kernel[i] <- sum((yy - density_sam1(xx))^2*interval)
  # The calculation of the ISE for the penalised kernal density estimator
  ise$second$OS[i] = integrate(function(x){
    (OS_Hermite(x, data = sample1) - density_sam1(x))^2},
    lower = min(sample1), upper = max(sample1))$value  
  ise$second$pkd[i] <- integrate(function(sample1){
    (dssden(sample.fit, sample1) - dnorm(sample1, mean = 0, sd = 1))^2}, 
    lower = min(sample1), upper = max(sample1))$value
}

# The Monte Carlo Simulation study for sample size n=1000
for(i in 1:repeat.no){
  sample1 <- rnorm(n[3], mean = mu, sd = sigma)
  sample.fit <- ssden( ~ sample1, domain = data.frame(sample1))
  # The calculation of the ISE for the kernel density estimator
  ker_den <- density(sample1)
  interval <- ker_den$x[-1] - ker_den$x[-length(ker_den$x)]
  xx <- (ker_den$x[-1] + ker_den$x[-length(ker_den$x)])/2
  yy <- (ker_den$y[-1] + ker_den$y[-length(ker_den$y)])/2
  ise$third$kernel[i] <- sum((yy - density_sam1(xx))^2*interval)
  # The calculation of the ISE for the penalised kernaldensity estimator
  ise$third$OS[i] <- integrate(function(x){
    (OS_Hermite(x, data = sample1) - density_sam1(x))^2},
    lower = min(sample1), upper = max(sample1))$value  
  ise$third$pkd[i] <- integrate(function(sample1){
    (dssden(sample.fit, sample1) - dnorm(sample1, mean = 0, sd = 1))^2}, 
    lower = min(sample1), upper = max(sample1))$value
}
```


```{r fig.cap="Boxplot of Different Estimators with Different Sample Sizes"}
par(mfrow=c(1,3))

boxplot(ise$first)
boxplot(ise$second)
boxplot(ise$third)
```


```{r fig.height=7, fig.width=10, fig.cap="Mean Integrated Squared Error with Different Sample Sizes"}
# Calculate the mean of the ISE of all the density estimators
ise.mean <- data.frame(kernel = c(mean(ise$first$kernel),
                                 mean(ise$second$kernel),
                                 mean(ise$third$kernel)),
                      OS = c(mean(ise$first$OS), 
                             mean(ise$second$OS), 
                             mean(ise$third$OS)),
                      pkd = c(mean(ise$first$pkd), 
                              mean(ise$second$pkd), 
                              mean(ise$third$pkd)),
                      row.names = c("first", "second", "third"))

plot(ise.mean$kernel, type = "b", col = "green", ylim = c(0,0.008), 
     xaxt = "n", xlab = "sample size")
axis(1, at = 1:3, labels = c(250, 500, 1000))
lines(ise.mean$OS, type = "b", col = "red")
lines(ise.mean$pkd, type = "b", col = "orange")

legend(x = "topright", 
       legend = c("Kernel Density Estimator",
                  "Orthogonal Series Estimator",
                  "Penalised Kernal Density Estimator"),
       lty = c(1, 1, 1),
       col = c("green", "red", "orange"), 
       lwd = 1.8, bty = "n")
```

# 5. Conclusion

\newpage

# Reference

1. Deng, H. and Wickham, H., 2011. Density estimation in R. Electronic publication.

2. Girolami, M., 2002. Orthogonal series density estimation and the kernel eigenvalue problem. Neural computation, 14(3), pp.669-688

3. Gu, C., 2011. Smoothing spline ANOVA models: R package gss. Journal of Statistical Software, 58, pp.1-25.

4. Kauermann, G. and Schellhase, C., 2019. Density Estimation with a Penalized Mixture Approach.

5. Kreyszig, E., 1991. Introductory functional analysis with applications (Vol. 17). John Wiley & Sons.

