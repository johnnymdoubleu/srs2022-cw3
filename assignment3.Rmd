---
title: |
  <center> University of Edinburgh, School of Mathematics </center>
  <center> Statistical Research Skills </center>
  <center> Assignment 3 - Simulation Report</center>
author: "Johnny Lee"
date: '26th Apr 2022 '
output:
  pdf_document : 
    fig_caption: true
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
Sys.setlocale("LC_ALL", "English")
library(tidyverse)
library(gss)
```

# 1. Introduction

This report consists of two main parts. In the first part, we aim to compute one-shot experiment on kernel density estimator and compare against its other competitors such as orthogonal series estimator and penalised kernel density estimator. Hence, we will describe the preliminary experiment in the first part. In the second part, we will conduct a Monte Carlo simulation study for different sample sizes on the previously suggested methods. Thus, we will evaluate the integrated squared error (ISE) for different cases of sample sizes. Then we will end this report in the conclusion.

# 2. Preliminary Experiment
This section consist of the first part of the report where we conduct the preliminary experiments with 3 different density estimators. We will introduce the detailed methodology with its mathematical equation. With random data generation from normal and beta distribution, we will conduct one-shot experiment on these estimators.

## 2.1 Methodology

### 2.1.1 Kernel Denisty Estimator
Let $X_1,\dots,X_n\overset{\text{iid}}{\sim}f$. The kernel density estimator of $f$ is defined as

$$\hat{f}(x)=\frac{1}{n}\sum^{n}_{i=1}K_h(x-X_i)$$
where $K_h(x)=\frac{K(x)}{h}$, $K$ is a kernel and $h>0$ is a parameter controlling smoothness of the estimate. Thus, we will perform an univariate density estimate with default kernel and bandwidth settings in this experiment.

### 2.1.2 Orthogonal Series Estimator 
With reference to (Kreyszig, 1991)[5], Kreyszig introduced the orthogonal series estimator using normalised Hermite polynomials. These polynomials form a orthonormal sequence for the univariate case as below,
$$\hat{f}(x)=\frac{1}{(2^ii!\sqrt{x})^{\frac{1}{2}}}\exp(-x^2/2)H_i(x)$$
and 
$$
H_0(x)=1, \quad H_i(x)=(-1)^i\exp(x^2)\frac{d^i}{dx^i}\exp(-x^2)
$$
where $H_i(x)$ is called the \textit{Hermite polynomial of order $i$} and orthogonal with respect to the probability density function. Furthermore, we can further simplify the process as Kreyszig showed its properties in a recurrence relation as below,
$$
H_{i+1}(x)=2xH_i(x)-H'_i(x), \quad H'_{i}(x)=2iH_{i-1}(x)
$$
Thus, we defined a new function to proceed to the iteration above to obtain the series of estimation.

```{r}
# The implementation of the orthogonal series estimator, using Hermite series
OS_Hermite <- function(x, data){
  # The Hermite polynomials
  # Utilize the recurrence formula "H_n(x) = 2x*H_(n-1)(x) - 2(n-1)*H_(n-2)(x) (n>=2)"
  # to calculate the Hermite polynomials
  hermite <- function(x, m){
    if(m == 0){
      1
    }
    else if(m == 1){2*x}
    else{
      a <- 1
      b <- 2*x
      for(i in 2:m){
        c <- 2*x*b - 2*(i - 1)*a
        a <- b
        b <- c
      }
      c
    }
  }
  # The normalized Hermite functions
  phi <- function(x, m){
    (hermite(x, m)/exp((x^2)/2))/sqrt((2^m)*factorial(m)*sqrt(pi))
  }
  n <- length(data)
  value = 0
  for(m in 0:30){
    c <- sum(sapply(data, phi, m = m))/n
    value <- value + c*phi(x, m)
  }
  value
}
```

### 2.1.3 Penalised Kernel Density Estimator
(Kauermann et al, 2009)[4] introduces the penalised likelihood 

$$
\hat{f}(x)=\sum^{m}_{n=-m}c_n\phi_n(x),
$$
where $\phi_n(x)$ is the basis densities. Then the weight $c_n$ is parameterised as follow
$$
c_n(y)=\frac{\exp(\beta_n)}{\sum^{m}_{n=-m}\exp(\beta_n)}
$$
with $\beta_0=0$ and $\pmb{\beta} = \beta_{-m}, \dots, \beta_{-1}, \beta_1, \dots\beta_m$ so that $\int f(x)dx=1$. (Deng et al, 2011)[1] then further suggested the simplified kernel approach that looks similar to kernel density estimator. The following expression shows the penalised approach for nonparametric density estimation for univariate case,
$$
\hat{f}(x)=\frac{1}{m}\sum^{m}_{n=1}K\bigg(\frac{x-\mu_n}{h}\bigg)
$$
where $\mu_i$ is a hyperparameter known as knots being placed on an equally spaced locations on the domain of the dataset. 
<!-- and introduced a package `gss`, (Gu, 2011)[3] which uses a penalised likelihood approach for nonparametric density estimation. With the functions `ssden()` and `dssden()`, we can estimate the kernel density. -->

## 2.2. Data Generating Process

```{r}
# Generate data for the experiment
n <- 100
set.seed(1)
# The sample one
sample1 <- rnorm(n, mean = 0, sd = 1)
#The sample two
sample2 <- rbeta(n, shape1 = 2, shape2 = 4)
```

We generated two random distributions, normal distribution and beta distribution. For the first distribution, we randomly generated $X\sim N(0,1)$ with $1000$ samples. Then the second distribution takes $X\sim Beta(2,4)$ with $1000$ samples. These values are chosen on a random basis and we are generating the data in a random process in base `R`.

## 2.3. One-shot Experiments 
Now we proceed to one-shot experiment on two randomly generated distribution in the previous section. First of all, one-shot experiment is the outcome produced by a single simulated data set. 

### 2.3.1. Normal Distribution

Let us look at Figure 1. The figure illustrates the true density with a histogram on the actual data set that we generated. Then we plotted 3 additional line that represent each of the estimators. By looking at the plot we can clearly observe that penalised kernel density estimator resembles the true density line the most compared to others. For the kernel density estimator, the peak is slightly on the right side of the distribution. On the other hand, orthogonal series estimator is shown to have a two hump and being less smoother compared to the rest. Overall, we can conclude that penalised kernel density estimator is the best among others in the one-shot experiment on normal distribution.
```{r fig.height=7.5, fig.width=12, fig.cap="One-shot Experiment on Normal distribution"}
set.seed(1)
# The density functions for the distributions of the simulated data
# The density function for the distribution of sample one
density1 <- function(x){
  mu <- 0; sigma <- 1
  exp(-(x - mu)^2/(2*sigma^2))/(sqrt(2*pi)*sigma)
}
# Create the visualizations to illustrate the performances of different density estimators
sample1.fit <- ssden( ~ sample1, domain = data.frame(sample1))
# The visualization based on sample one
xx <- seq(min(sample1), max(sample1), len = n)
# pdf("Part1_fig1.pdf")
hist(sample1, freq = FALSE, main = "",
     xlab = "X", ylab = "Density", xlim = c(-3, 3), ylim = c(0, 0.6))
lines(density(sample1, from = min(sample1), to = max(sample1)), 
      col = "green", lwd = 1.6)
lines(xx, density1(xx), type = "l", col = "black", lty = 2, lwd = 1.6)
# axis(side = 1, at = seq(min(sample1), max(sample1), n))
lines(xx, sapply(xx, OS_Hermite, data = sample1), col = "red", lwd = 1.6)
lines(xx, dssden(sample1.fit, xx), type = "l", col = "orange", lwd = 1.6)
legend(x = "topright", 
       legend = c("True Density", 
                  "Kernel Density Estimator",
                  "Orthogonal Series Estimator",
                  "Penalised Kernal Density Estimator"),
       lty = c(2, 1, 1, 1),
       col = c("black", "green", "red", "orange"),
       lwd = 1.8, bty = "n")
```

### 2.3.2. Beta Distribution
```{r fig.height=7.5, fig.width=12, fig.cap="One-shot Experiment on Beta distribution"}
# The visualization based on sample two
# The density function for the distribution of sample two
density2 <- function(x){
  alpha <- 2; beta <- 4
  x^(alpha - 1)*(1 - x)^(beta - 1)/beta(alpha, beta)
}
sample2.fit <- ssden( ~ sample2, domain = data.frame(sample2))
xx <- seq(min(sample2), max(sample2), len = n)
hist(sample2, freq = FALSE, main="",
     xlab = "X", ylab = "Density", xlim = c(0, 0.8), ylim = c(0, 3.8))
lines(density(sample2, from = min(sample2), to = max(sample2)), 
      col = "green", lwd = 1.6)
lines(xx, density2(xx), type = "l", col = "black", lty = 2, lwd = 1.6)
# axis(side = 1, at = seq(min(sample2), max(sample2), n))
lines(xx, sapply(xx, OS_Hermite, data = sample2), col = "red", lwd = 1.6)
lines(xx, dssden(sample2.fit, xx), type = "l", col = "orange", lwd = 1.6)
legend(x = "topright", 
       legend = c("True Density", 
                  "Kernel Density Estimator",
                  "Orthogonal Series Estimator",
                  "Penalised Kernal Density Estimator"),
       lty = c(2, 1, 1, 1),
       col = c("black", "green", "red", "orange"), 
       lwd = 1.8, bty = "n")
```

# 3. Monte Carlo Simulation Study

Now we move onto the Monte Carlo Simulation Study, where we repeat the one-shot experiment $R$ times, for different simulated dataset. This brings advantage in concluding with a solid result and logic. Hence, we repeated this process with $100$ times on the randomly generated $X\sim N(0,1)$ on different sample sizes. To that, we computed the integrated squared error,
$$
ISE = \int\{\hat{f}(x)-f(x)\}^2
$$
to compare the performances of each density estimator on different sample sizes. Note that for the reproduceability, we fixed a seed in `R` using `set.seed(1)`.  
```{r}
set.seed(1)
# Initialize some variables
mu <- 0; sigma <- 1
repeat.no <- 10
n <- c(250, 500, 1000)
df <- 5
ise <- list()
ise$second <- ise$first <- data.frame("kernel" = rep(0, repeat.no), 
                                      "OS" = rep(0, repeat.no),
                                      "pkd" = rep(0, repeat.no))
ise$third <- ise$first

# The density function of the distribution of the sample
density_sam1 = function(x){
  exp(-(x - mu)^2/(2*sigma^2))/(sqrt(2*pi)*sigma)
}
# The Monte Carlo Simulation study for sample size n=250
for(i in 1:repeat.no){
  sample1 <- rnorm(n[1], mean = mu, sd = sigma)
  sample.fit <- ssden( ~ sample1, domain = data.frame(sample1))
  # The calculation of the ISE for the kernel density estimator
  ker_den <- density(sample1)
  interval <- ker_den$x[-1] - ker_den$x[-length(ker_den$x)]
  xx <- (ker_den$x[-1] + ker_den$x[-length(ker_den$x)])/2
  yy <- (ker_den$y[-1] + ker_den$y[-length(ker_den$y)])/2
  ise$first$kernel[i] <- sum((yy - density_sam1(xx))^2*interval)
  
  # The calculation of the ISE for the penalised kernal density estimator
  ise$first$OS[i] <- integrate(function(x){
    (OS_Hermite(x, data = sample1) - density_sam1(x))^2},
    lower = min(sample1), upper = max(sample1))$value 
  
  ise$first$pkd[i] <- integrate(function(sample1){
    (dssden(sample.fit, sample1) - dnorm(sample1, mean = 0, sd = 1))^2}, 
    lower = min(sample1), upper = max(sample1))$value
}

# The Monte Carlo Simulation study for sample size n=500
for(i in 1:repeat.no){
  sample1 <- rnorm(n[2], mean = mu, sd = sigma)
  sample.fit <- ssden( ~ sample1, domain = data.frame(sample1))
  # The calculation of the ISE for the kernel density estimator
  ker_den <- density(sample1)
  interval <- ker_den$x[-1] - ker_den$x[-length(ker_den$x)]
  xx <- (ker_den$x[-1] + ker_den$x[-length(ker_den$x)])/2
  yy <- (ker_den$y[-1] + ker_den$y[-length(ker_den$y)])/2
  ise$second$kernel[i] <- sum((yy - density_sam1(xx))^2*interval)
  # The calculation of the ISE for the penalised kernal density estimator
  ise$second$OS[i] = integrate(function(x){
    (OS_Hermite(x, data = sample1) - density_sam1(x))^2},
    lower = min(sample1), upper = max(sample1))$value  
  ise$second$pkd[i] <- integrate(function(sample1){
    (dssden(sample.fit, sample1) - dnorm(sample1, mean = 0, sd = 1))^2}, 
    lower = min(sample1), upper = max(sample1))$value
}

# The Monte Carlo Simulation study for sample size n=1000
for(i in 1:repeat.no){
  sample1 <- rnorm(n[3], mean = mu, sd = sigma)
  sample.fit <- ssden( ~ sample1, domain = data.frame(sample1))
  # The calculation of the ISE for the kernel density estimator
  ker_den <- density(sample1)
  interval <- ker_den$x[-1] - ker_den$x[-length(ker_den$x)]
  xx <- (ker_den$x[-1] + ker_den$x[-length(ker_den$x)])/2
  yy <- (ker_den$y[-1] + ker_den$y[-length(ker_den$y)])/2
  ise$third$kernel[i] <- sum((yy - density_sam1(xx))^2*interval)
  # The calculation of the ISE for the penalised kernaldensity estimator
  ise$third$OS[i] <- integrate(function(x){
    (OS_Hermite(x, data = sample1) - density_sam1(x))^2},
    lower = min(sample1), upper = max(sample1))$value  
  ise$third$pkd[i] <- integrate(function(sample1){
    (dssden(sample.fit, sample1) - dnorm(sample1, mean = 0, sd = 1))^2}, 
    lower = min(sample1), upper = max(sample1))$value
}
```


```{r fig.cap="Boxplot of Different Estimators with Different Sample Sizes"}
par(mfrow=c(1,3))

boxplot(ise$first)
boxplot(ise$second)
boxplot(ise$third)
```


```{r fig.cap="Mean Integrated Squared Error with Different Sample Sizes"}
# Calculate the mean of the ISE of all the density estimators
ise.mean <- data.frame(kernel = c(mean(ise$first$kernel),
                                 mean(ise$second$kernel),
                                 mean(ise$third$kernel)),
                      OS = c(mean(ise$first$OS), 
                             mean(ise$second$OS), 
                             mean(ise$third$OS)),
                      pkd = c(mean(ise$first$pkd), 
                              mean(ise$second$pkd), 
                              mean(ise$third$pkd)),
                      row.names = c("first", "second", "third"))

plot(ise.mean$kernel, type = "b", col = "green", ylim = c(0,0.008), 
     xaxt = "n", xlab = "sample size")
axis(1, at = 1:3, labels = c(250, 500, 1000))
lines(ise.mean$OS, type = "b", col = "red")
lines(ise.mean$pkd, type = "b", col = "orange")

legend(x = "topright", 
       legend = c("Kernel Density Estimator",
                  "Orthogonal Series Estimator",
                  "Penalised Kernal Density Estimator"),
       lty = c(1, 1, 1),
       col = c("green", "red", "orange"), 
       lwd = 1.8, bty = "n")
```

# 4. Conclusion

\newpage

# Reference

1. Deng, H. and Wickham, H., 2011. Density estimation in R. Electronic publication.

2. Girolami, M., 2002. Orthogonal series density estimation and the kernel eigenvalue problem. Neural computation, 14(3), pp.669-688

<!-- 3. Gu, C., 2011. Smoothing spline ANOVA models: R package gss. Journal of Statistical Software, 58, pp.1-25. -->

3. Kauermann, G. and Schellhase, C., 2019. Density Estimation with a Penalized Mixture Approach.

4. Kreyszig, E., 1991. Introductory functional analysis with applications (Vol. 17). John Wiley & Sons.

